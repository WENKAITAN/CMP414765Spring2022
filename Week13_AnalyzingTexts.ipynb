{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week13_AnalyzingTexts",
      "provenance": [],
      "authorship_tag": "ABX9TyN/65WJscirI7MozQGqBCXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Spring2022/blob/main/Week13_AnalyzingTexts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVw7vPwMiKe4"
      },
      "source": [
        "# Week 13\n",
        "# Analyzing Texts\n",
        "\n",
        "This notebook classifies movie reviews as positive or negative using the text of the review.\n",
        "\n",
        "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the Internet Movie Database. These reviews are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n",
        "\n",
        "**Please turn on GPU computing from the menu.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVtn0DIYozEc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neys5y0Fo7ee"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c2ZI4wxo-2h"
      },
      "source": [
        "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgstHu37pAMM"
      },
      "source": [
        "## Explore the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl6KiQaBpBqS"
      },
      "source": [
        "?train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXB51ZbFpDMf"
      },
      "source": [
        "# Extract the first batch of 10 reviews\n",
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10))) # The next() function returns the next item of an iterator\n",
        "train_examples_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FbgbCFtpEiq"
      },
      "source": [
        "# Display the labels of the first 10 reviews\n",
        "train_labels_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzQuawK7pHEF"
      },
      "source": [
        "## Building the Model\n",
        "- Represent words as vectors using pre-trained encoder\n",
        "- Decide the number of hidden layers\n",
        "- Decide the number of hidden units for each layer\n",
        "\n",
        "For this example we will use a pre-trained text embedding model from TensorFlow Hub called `gnews-swivel-20dim`, which represents each word with a vector of length 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RvLWpkppkzy"
      },
      "source": [
        "# Word Embedding\n",
        "\n",
        "## Why transform words into vectors?\n",
        "\n",
        "## Challenges for word embedding\n",
        "- curse of dimensionality\n",
        "- performance metrics\n",
        "- training algorithm\n",
        "\n",
        "# Popular embedding models\n",
        "- Word2Vec\n",
        "- BERT\n",
        "- Train your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWWRoUmwpI19"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0K0PD3npK4z"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnB-E4jrpMQH"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2Wn9yPApNpO"
      },
      "source": [
        "# the fit() methods returns a collection of intermediate results, which can be useful\n",
        "# to evaluate the model\n",
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GBeeoPNpOqD"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y89oOk3TpQCl"
      },
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOoxBctJpQ4Y"
      },
      "source": [
        "# How about my own reviews?\n",
        "my_review = np.array([\"This movie is the worst action movie I have ever watched in my entire life.\",\n",
        "                      \"I really enjoyed the plot, but the lead actor didn't portray his character well.\",\n",
        "                      \"It is the most visually stunning movie in the series. The acting is outstanding too.\",\n",
        "                      \"I really like that everyone in this movie makes it crystal clear that they don't care the quality at all.\",\n",
        "                      \"There is nothing about the movie that I don't like. I wish everyone else just stop making movies since no moive can be better than this one.\"])\n",
        "model(my_review).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpvE5owlpR6t"
      },
      "source": [
        "# Extract 20 reviews from the test set\n",
        "reviews, labels = next(iter(test_data.batch(20)))\n",
        "predictions = model(reviews).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoXFpWopUBj"
      },
      "source": [
        "labels.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzhso8YLpVTj"
      },
      "source": [
        "(predictions > 0).astype(int).reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxLpa40JpYfC"
      },
      "source": [
        "reviews[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pIMD5sgpZls"
      },
      "source": [
        "# Text Generation with Recurrent Neural Networks\n",
        "\n",
        "## Idea\n",
        "- In some applications, data arrive in a sequence.\n",
        "- Output is context-dependent, so each node should remember its previous status.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/tutorials/text/images/text_generation_training.png\" width=\"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KYtGp4mqGW4"
      },
      "source": [
        "- We will work with a dataset of Shakespeare's writing\n",
        "- Build a model with `tf.keras` to analyze the sequence of characters\n",
        "- Apply the model to write new text in Shakespeare's style\n",
        "\n",
        "This project is adapted from [TensorFlow tutorial](https://www.tensorflow.org/tutorials/sequences/text_generation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtEfFP9UqO5W"
      },
      "source": [
        "# Get the text file from:\n",
        "# https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R4WxCndqQH3"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k0YXgsJqRtO"
      },
      "source": [
        "# Read the text as a string\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_FcvfJXqSfF"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSz5kAx5qTbM"
      },
      "source": [
        "# The unique characters in the file\n",
        "# Python set is a data structure containing unique elements\n",
        "vocab = sorted(set(text))\n",
        "# Print with formatted string: {index:format}\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay6BYkhMqU8c"
      },
      "source": [
        "## Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX0LNVbXqW2M"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "# enumerate: returns index and the value\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbCDcqO4qXtf"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lluJPO73qYyA"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "# repr: string representation of an object\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(str(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg4UPSI3qait"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "# Dataset.from_tensor_slices(): convert a numpy array to tf Dataset\n",
        "# Dataset.take(): create a sub Dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLXdzmq3qbaD"
      },
      "source": [
        "# batch(): cut the dataset into chunks\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# join(): concatenate a list of elements and form a string\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EI2wJWnqc3f"
      },
      "source": [
        "## Match input character with output character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zYrBjlfqeVu"
      },
      "source": [
        "# For each sequence, duplicate and shift it to form the input and target text by using the `map` method\n",
        "# to apply a simple function to each batch\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# map(): similar to pandas.DataFrame.apply()\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1EelxrnqfSu"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmuCrCPbqgdn"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cJVzuCaqhjD"
      },
      "source": [
        "## Shuffle and create training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFHcXnXcqjd0"
      },
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqV8PBKnqkXL"
      },
      "source": [
        "## Build the training model\n",
        "\n",
        "- Embedding layer\n",
        "- GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II1bmBMMqmiJ"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPlx8caQqnsx"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7x02NPJqofP"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENBMP5y6qpc9"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNxb2yYxqqNW"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4xFc2cqrQx"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8aTtDYRqsM8"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roECwgcXqtNe"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXqO7MoIquL4"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHu-pIhYqvTS"
      },
      "source": [
        "import os\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3bXZe25qwNO"
      },
      "source": [
        "EPOCHS=10\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynLMZ6WNqxN6"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UXH7Hx5q2Jx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}