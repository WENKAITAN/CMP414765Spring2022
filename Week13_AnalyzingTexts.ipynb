{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week13_AnalyzingTexts",
      "provenance": [],
      "authorship_tag": "ABX9TyNPeqiizcol7tpNuaxEtVzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Spring2022/blob/main/Week13_AnalyzingTexts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVw7vPwMiKe4"
      },
      "source": [
        "# Week 13\n",
        "# Analyzing Texts\n",
        "\n",
        "This notebook classifies movie reviews as positive or negative using the text of the review.\n",
        "\n",
        "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the Internet Movie Database. These reviews are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n",
        "\n",
        "**Please turn on GPU computing from the menu.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVtn0DIYozEc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neys5y0Fo7ee"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c2ZI4wxo-2h"
      },
      "source": [
        "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgstHu37pAMM"
      },
      "source": [
        "## Explore the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl6KiQaBpBqS"
      },
      "source": [
        "?train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXB51ZbFpDMf"
      },
      "source": [
        "# Extract the first batch of 10 reviews\n",
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10))) # The next() function returns the next item of an iterator\n",
        "train_examples_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FbgbCFtpEiq"
      },
      "source": [
        "# Display the labels of the first 10 reviews\n",
        "train_labels_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzQuawK7pHEF"
      },
      "source": [
        "## Building the Model\n",
        "- Represent words as vectors using pre-trained encoder\n",
        "- Decide the number of hidden layers\n",
        "- Decide the number of hidden units for each layer\n",
        "\n",
        "For this example we will use a pre-trained text embedding model from TensorFlow Hub called `gnews-swivel-20dim`, which represents each word with a vector of length 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RvLWpkppkzy"
      },
      "source": [
        "# Word Embedding\n",
        "\n",
        "## Why transform words into vectors?\n",
        "\n",
        "## Challenges for word embedding\n",
        "- curse of dimensionality\n",
        "- performance metrics\n",
        "- training algorithm\n",
        "\n",
        "# Popular embedding models\n",
        "- Word2Vec\n",
        "- BERT\n",
        "- Train your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWWRoUmwpI19"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0K0PD3npK4z"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnB-E4jrpMQH"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2Wn9yPApNpO"
      },
      "source": [
        "# the fit() methods returns a collection of intermediate results, which can be useful\n",
        "# to evaluate the model\n",
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GBeeoPNpOqD"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y89oOk3TpQCl"
      },
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOoxBctJpQ4Y"
      },
      "source": [
        "# How about my own reviews?\n",
        "my_review = np.array([\"This movie is the worst action movie I have ever watched in my entire life.\",\n",
        "                      \"I really enjoyed the plot, but the lead actor didn't portray his character well.\",\n",
        "                      \"It is the most visually stunning movie in the series. The acting is outstanding too.\",\n",
        "                      \"I really like that everyone in this movie makes it crystal clear that they don't care the quality at all.\",\n",
        "                      \"There is nothing about the movie that I don't like. I wish everyone else just stop making movies since no moive can be better than this one.\"])\n",
        "model(my_review).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpvE5owlpR6t"
      },
      "source": [
        "# Extract 20 reviews from the test set\n",
        "reviews, labels = next(iter(test_data.batch(20)))\n",
        "predictions = model(reviews).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoXFpWopUBj"
      },
      "source": [
        "labels.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzhso8YLpVTj"
      },
      "source": [
        "(predictions > 0).astype(int).reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxLpa40JpYfC"
      },
      "source": [
        "reviews[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}